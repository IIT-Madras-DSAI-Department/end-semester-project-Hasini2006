# -*- coding: utf-8 -*-
"""Algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rYKyY7vsynv1BJll0H_q4BaZ40g2Wmn2
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay, f1_score
import time

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

"""# Import datasets"""

train = pd.read_csv('/content/drive/MyDrive/ml lab endsemm/MNIST_train.csv')
val = pd.read_csv('/content/drive/MyDrive/ml lab endsemm/MNIST_validation.csv')
train.head()

"""# Separate target col and standardise"""

train_x = train.drop(columns=['label','even'])
train_y = train['label']
val_x = val.drop(columns=['label','even'])
val_y = val['label']

train_x = train_x.to_numpy().astype(np.float64)/255.0
train_y = train_y.to_numpy()
val_x = val_x.to_numpy().astype(np.float64)/255.0
val_y= val_y.to_numpy()

"""# PCA implementation"""

class PCAModel:
    def __init__(self, n_components):
        self.n_components = n_components
        self.mean = None
        self.components = None
        self.explained_variance = None

    def fit(self, X):
        X = np.array(X, dtype=float)
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        cov_matrix = np.cov(X_centered, rowvar=False)
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        sorted_idx = np.argsort(eigenvalues)[::-1]
        self.explained_variance = eigenvalues[sorted_idx][:self.n_components]
        self.components = eigenvectors[:, sorted_idx][:, :self.n_components]

    def predict(self, X):
        if self.mean is None or self.components is None:
            raise ValueError("The PCA model has not been fitted yet.")
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)

    def reconstruct(self, X):
        Z = self.predict(X)

        return np.dot(Z, self.components.T) + self.mean

    def detect_anomalies(self, X, threshold=None, return_errors=False):
        X_reconstructed = self.reconstruct(X)

        errors = np.mean((X - X_reconstructed) ** 2, axis=1)

        if threshold is None:
            threshold = np.percentile(errors, 95)

        flag = errors > threshold

        is_anomaly = flag * 1

        return is_anomaly, errors

"""# Tuning of no.of components"""

components_list = [1, 2, 5, 10, 20, 50, 100, 200, 300, 400, 450, 500, 550, 600]
errors_dict = {}
for n in components_list:
    model_pca = PCAModel(n_components=n)
    model_pca.fit(train_x)
    X_reconstructed = model_pca.reconstruct(train_x)
    errors = np.mean((train_x - X_reconstructed) ** 2, axis=1)
    avg_error = np.mean(errors)
    errors_dict[n] = avg_error
for k, err in errors_dict.items():
    print(f"{k} components -> {err:.6f}")

"""# Consider top 500 components"""

pca = PCAModel(n_components=500)
pca.fit(train_x)
train_x_pca = pca.predict(train_x)
val_x_pca = pca.predict(val_x)

"""# 1.SoftMax Classification"""

class SoftmaxRegression:
    def __init__(self, learning_rate=0.1, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.W = None
        self.b = None
    def _softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def _one_hot(self, y, num_classes):
        return np.eye(num_classes)[y]

    def _cross_entropy_loss(self, y_true, y_pred):
        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))

    def fit(self, X, y):
        num_samples, num_features = X.shape
        num_classes = np.max(y) + 1
        self.W = np.random.randn(num_features, num_classes) * 0.01
        self.b = np.zeros((1, num_classes))
        Y_onehot = self._one_hot(y, num_classes)
        for epoch in range(self.epochs):
            # Forward pass
            logits = np.dot(X, self.W) + self.b
            probs = self._softmax(logits)
            # Loss (for monitoring)
            loss = self._cross_entropy_loss(Y_onehot, probs)
            grad_logits = (1./ num_samples) * (Y_onehot - probs)
            grad_W = -np.dot(X.T, grad_logits)
            grad_b = -np.sum(grad_logits, axis=0, keepdims=True)
            self.W -= self.learning_rate * grad_W
            self.b -= self.learning_rate * grad_b

            if epoch % 100 == 0 or epoch == self.epochs - 1:
                print(f"Epoch {epoch}: Loss = {loss:.4f}")

    def predict_proba(self, X):
        logits = np.dot(X, self.W) + self.b
        return self._softmax(logits)

    def predict(self, X):
        probs = self.predict_proba(X)
        return np.argmax(probs, axis=1)

model = SoftmaxRegression(learning_rate=0.1, epochs=500)
model.fit(train_x_pca, train_y)
val_pred = model.predict(val_x_pca)
accuracy = np.mean(val_pred == val_y)
print("Validation Accuracy:", accuracy)
val_proba = model.predict_proba(val_x_pca)
# One-hot true labels
num_classes = np.max(train_y) + 1
val_y_onehot = np.eye(num_classes)[val_y]
bias = np.mean(np.sum((val_proba - val_y_onehot)**2, axis=1))
print("Bias:", bias)
variance = np.mean(np.var(val_proba, axis=1))
print("Variance:", variance)
ConfusionMatrixDisplay(confusion_matrix(val_y, val_pred)).plot()

model = SoftmaxRegression(learning_rate=0.1, epochs=500)
model.fit(train_x, train_y)
val_pred = model.predict(val_x)
accuracy = np.mean(val_pred == val_y)
print("Validation Accuracy:", accuracy)
val_proba = model.predict_proba(val_x)
num_classes = np.max(train_y) + 1
val_y_onehot = np.eye(num_classes)[val_y]

bias = np.mean(np.sum((val_proba - val_y_onehot)**2, axis=1))
print("Bias:", bias)

variance = np.mean(np.var(val_proba, axis=1))
print("Variance:", variance)
ConfusionMatrixDisplay(confusion_matrix(val_y, val_pred)).plot()

for epochs in [200, 400, 500, 700, 900, 1200]:
    model = SoftmaxRegression(learning_rate=0.1, epochs=epochs)
    start_time = time.time()
    model.fit(train_x, train_y)
    val_pred = model.predict(val_x)
    end_time = time.time()
    accuracy = np.mean(val_pred == val_y)

    print("Validation Accuracy:", accuracy)
    print('F1 score:', f1_score(val_y, val_pred, average='macro'))
    print('Time taken(s):', end_time-start_time)
    val_proba = model.predict_proba(val_x)
    num_classes = np.max(train_y) + 1
    val_y_onehot = np.eye(num_classes)[val_y]

    bias = np.mean(np.sum((val_proba - val_y_onehot)**2, axis=1))
    print("Bias:", bias)

    variance = np.mean(np.var(val_proba, axis=1))
    print("Variance:", variance)

for epochs in [900, 1000, 1500]:
  for l in [0.01, 0.1]:
    model = SoftmaxRegression(learning_rate=l, epochs=epochs)
    start_time = time.time()
    model.fit(train_x_pca, train_y)
    val_pred = model.predict(val_x_pca)
    end_time = time.time()
    print(f'Time(s):',end_time - start_time)
    accuracy = np.mean(val_pred == val_y)
    print("Validation Accuracy:", accuracy)
    print('F1 score:', f1_score(val_y, val_pred, average='macro'))
    val_proba = model.predict_proba(val_x_pca)
    num_classes = np.max(train_y) + 1
    val_y_onehot = np.eye(num_classes)[val_y]
    bias = np.mean(np.sum((val_proba - val_y_onehot)**2, axis=1))
    print("Bias:", bias)
    variance = np.mean(np.var(val_proba, axis=1))
    print("Variance:", variance)

for epochs in [900, 1000, 1500]:
  for l in [0.01, 0.1]:
    model = SoftmaxRegression(learning_rate=l, epochs=epochs)
    start_time = time.time()
    model.fit(train_x, train_y)
    val_pred = model.predict(val_x)
    end_time = time.time()
    print(f'Time(s):',end_time - start_time)
    accuracy = np.mean(val_pred == val_y)
    print("Validation Accuracy:", accuracy)
    print('F1 score:', f1_score(val_y, val_pred, average='macro'))
    val_proba = model.predict_proba(val_x)
    num_classes = np.max(train_y) + 1
    val_y_onehot = np.eye(num_classes)[val_y]
    bias = np.mean(np.sum((val_proba - val_y_onehot)**2, axis=1))
    print("Bias:", bias)
    variance = np.mean(np.var(val_proba, axis=1))
    print("Variance:", variance)

"""# KNN"""

class KNNClassifier:
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def _euclidean_distance(self, a, b):
        return np.sqrt(np.sum((a - b)**2, axis=1))

    def predict_one(self, x):
        distances = self._euclidean_distance(self.X_train, x)
        k_idx = np.argsort(distances)[:self.k]
        k_labels = self.y_train[k_idx]
        return np.bincount(k_labels).argmax()

    def predict(self, X):
        return np.array([self.predict_one(x) for x in X])

k_values = [3,5,7,9,11,15]
for k in k_values:
    knn = KNNClassifier(k=k)
    knn.fit(train_x_pca, train_y)
    # Predict on validation
    start_time = time.time()
    val_pred = knn.predict(val_x_pca)
    end_time = time.time()
    # Accuracy
    accuracy = np.mean(val_pred == val_y)
    print("KNN Validation Accuracy:", accuracy)
    print("f1_score:", f1_score(val_y, val_pred, average='macro'))
    print('Time taken(s):', end_time-start_time)

k_values = [3,5,7,9,11,15]
for k in k_values:
    knn = KNNClassifier(k=k)
    knn.fit(train_x, train_y)
    # Predict on validation
    start_time = time.time()
    val_pred = knn.predict(val_x)
    end_time = time.time()
    # Accuracy
    accuracy = np.mean(val_pred == val_y)
    print("KNN Validation Accuracy:", accuracy)
    print("f1_score:", f1_score(val_y, val_pred, average='macro'))
    print('Time taken(s):', end_time-start_time)

"""# Boosting"""

class tree_node:
    def __init__(self, feature_idx=None, threshold=None, left_node=None, right_node=None, leaf_value=None):
        self.feature_idx = feature_idx
        self.threshold = threshold
        self.left_node = left_node
        self.right_node = right_node
        self.leaf_value = leaf_value

class XGBoostMultiClass:
    def __init__(self, num_classes, n_estimators=200, learning_rate=0.2, max_depth=5):
        self.K = num_classes
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []
        self.init_scores = None

    def softmax(self, logits):
        logits = logits - np.max(logits, axis=1, keepdims=True)
        exp_vals = np.exp(logits)
        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)

    def build(self, X, grad, hess, depth):
        if depth >= self.max_depth or len(X) == 0:
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        G_total, H_total = np.sum(grad), np.sum(hess)
        best_gain = -float('inf')
        best_feat = None
        best_thresh = None
        best_left = best_right = None

        feature_count = int(np.sqrt(X.shape[1])) + 1
        feature_indices = np.random.choice(X.shape[1], feature_count, replace=False)

        for j in feature_indices:
            values = np.unique(X[:, j])
            thresholds = np.random.choice(values, min(10, len(values)), replace=False)

            for threshold in thresholds:
                left = X[:, j] <= threshold
                right = ~left

                if not np.any(left) or not np.any(right):
                    continue

                G_l, H_l = np.sum(grad[left]), np.sum(hess[left])
                G_r, H_r = np.sum(grad[right]), np.sum(hess[right])

                gain = 0.5 * (
                    G_l**2 / (H_l + 1e-8) +
                    G_r**2 / (H_r + 1e-8) -
                    G_total**2 / (H_total + 1e-8)
                )

                if gain > best_gain:
                    best_gain = gain
                    best_feat = j
                    best_thresh = threshold
                    best_left = left
                    best_right = right

        if best_gain == -float('inf'):
            leaf_value = -np.sum(grad) / (np.sum(hess) + 1e-8)
            return tree_node(leaf_value=leaf_value)

        left_node = self.build(X[best_left],  grad[best_left],  hess[best_left],  depth+1)
        right_node = self.build(X[best_right], grad[best_right], hess[best_right], depth+1)

        return tree_node(feature_idx=best_feat, threshold=best_thresh,
                         left_node=left_node, right_node=right_node)

    def pred_one(self, x, node):
        while node.leaf_value is None:
            if x[node.feature_idx] <= node.threshold:
                node = node.left_node
            else:
                node = node.right_node
        return node.leaf_value
    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)
        N = len(y)

        Y = np.eye(self.K)[y]

        scores = np.zeros((N, self.K))
        self.init_scores = np.zeros(self.K)

        self.trees = []

        for m in range(self.n_estimators):
            probs = self.softmax(scores)
            grad = probs - Y
            hess = probs * (1 - probs)

            round_trees = []
            for k in range(self.K):
                tree = self.build(X, grad[:, k], hess[:, k], depth=0)
                update = np.array([self.pred_one(row, tree) for row in X])
                scores[:, k] += self.learning_rate * update

                round_trees.append(tree)

            self.trees.append(round_trees)
    def predict(self, X):
        X = np.asarray(X)
        N = X.shape[0]

        scores = np.zeros((N, self.K))

        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update

        probs = self.softmax(scores)
        return np.argmax(probs, axis=1)

    def predict_proba(self, X):
        X = np.asarray(X)
        N = X.shape[0]

        scores = np.zeros((N, self.K))

        for round_trees in self.trees:
            for k in range(self.K):
                update = np.array([self.pred_one(row, round_trees[k]) for row in X])
                scores[:, k] += self.learning_rate * update

        return self.softmax(scores)

model = XGBoostMultiClass(num_classes=10, n_estimators=50, learning_rate=0.2, max_depth=5)
starttime = time.time()
model.fit(train_x, train_y)
ypred = model.predict(val_x)
print("Time taken:", time.time() - starttime)
print("Accuracy:", accuracy_score(val_y, ypred))
print("F1 Score:", f1_score(val_y, ypred, average='macro'))

for n in [50, 100]:
  for d in [3, 5]:
    model = XGBoostMultiClass(num_classes=10, n_estimators=n, learning_rate=0.2, max_depth=d)
    starttime = time.time()
    model.fit(train_x, train_y)
    ypred = model.predict(val_x)
    print("Time taken:", time.time() - starttime)
    print("Accuracy:", accuracy_score(val_y, ypred))
    print("F1 Score:", f1_score(val_y, ypred, average='macro'))

"""# Bagging"""

from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def gini(self, y):
        m = len(y)
        if m == 0:
            return 0
        counts = np.bincount(y)
        probs = counts / m
        return 1 - np.sum(probs ** 2)

    def best_split(self, X, y):
        m, n = X.shape
        best_feature, best_thresh = None, None
        best_gini = float('inf')

        for feature in range(n):
            thresholds = np.unique(X[:, feature])
            for t in thresholds:
                left = X[:, feature] <= t
                right = ~left

                if left.sum() == 0 or right.sum() == 0:
                    continue

                g_left = self.gini(y[left])
                g_right = self.gini(y[right])

                weighted = (left.sum()*g_left + right.sum()*g_right) / m

                if weighted < best_gini:
                    best_gini = weighted
                    best_feature = feature
                    best_thresh = t

        return best_feature, best_thresh

    def build_tree(self, X, y, depth=0):
        m, n = X.shape
        num_classes = len(np.unique(y))

        if depth >= self.max_depth or num_classes == 1 or m < self.min_samples_split:
            return Counter(y).most_common(1)[0][0]

        feature, thresh = self.best_split(X, y)
        if feature is None:
            return Counter(y).most_common(1)[0][0]

        left = X[:, feature] <= thresh
        right = ~left

        left_branch = self.build_tree(X[left], y[left], depth+1)
        right_branch = self.build_tree(X[right], y[right], depth+1)

        return {"feature": feature, "threshold": thresh,
                "left": left_branch, "right": right_branch}

    def fit(self, X, y):
        self.tree = self.build_tree(X, y)

    def predict_one(self, x, node):
        if not isinstance(node, dict):
            return node
        if x[node["feature"]] <= node["threshold"]:
            return self.predict_one(x, node["left"])
        else:
            return self.predict_one(x, node["right"])

    def predict(self, X):
        return np.array([self.predict_one(x, self.tree) for x in X])

class BaggingClassifier:
    def __init__(self, n_estimators=10, max_depth=10):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.trees = []

    def bootstrap(self, X, y):
        m = len(X)
        idx = np.random.choice(m, m, replace=True)
        return X[idx], y[idx]

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_estimators):
            X_s, y_s = self.bootstrap(X, y)
            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X_s, y_s)
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        # shape = (n_trees, n_samples)

        final_preds = []
        for i in range(tree_preds.shape[1]):
            votes = Counter(tree_preds[:, i])
            final_preds.append(votes.most_common(1)[0][0])

        return np.array(final_preds)

bag = BaggingClassifier(n_estimators=20, max_depth=12)
bag.fit(train_x, train_y)
pred = bag.predict(val_x)
print("Accuracy:", accuracy_score(val_y, pred))
print("F1 Score:", f1_score(val_y, pred, average='macro'))

